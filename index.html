<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<script type="text/front-matter">
  title: "Article Title"
  description: "Description of the post"
  authors:
  - Logan Engstrom: http://loganengstrom.com/
  - Andrew Ilyas: http://andrewilyas.com/
  - Aleksander Madry: https://people.csail.mit.edu/madry/
  - Shibani Santurkar: http://people.csail.mit.edu/shibani/
  - Brandon Tran: 
  - Dimitris Tsipras: http://people.csail.mit.edu/tsipras/
  affiliations:
  - MIT
  - MIT
  - MIT
  - MIT
  - MIT
  - MIT 
</script>

<style>
    h3 {
	font-style: normal !important;
    }
</style>

<dt-article>
  <h1>Hello World</h1>
  <h2>A description of the article</h2>
  <dt-byline></dt-byline>
  <h2>Main points</h2>
  
  <h3><i>Takeaway #1:</i> Adversarial examples as innate
      brittleness vs. useful features (sensitivity vs reliance)</h3>
  <p>The goal of our experiments with non-robust features is to understand
  how adversarial examples fit into the following two worlds:
<ul>
    <li><b>World 1: Adversarial examples exploit directions irrelevant for
	classification.</b> In this world, adversarial examples arise from
    sensitivity to a signal that is unimportant for classification. For
    instance, suppose there is a feature \(f(x)\) that is not generalizing
    on the data<dt-fn>Note that \(f(x)\) could be correlated with the label
	in the training set but not in expectation/on the test
	set.</dt-fn>, but the model for some reason puts a lot of weight on
    it, <i> i.e., this sensitivity is an aberration "hallucinated" by the
	model </i>. Adversarial examples correspond to perturbing the input
    to change this feature by a small amount. This perturbation, however,
    would be orthogonal to how the model actually typically makes
    predictions (on natural data). (Note that this is just a single
    illustrative example---the key characteristic of this world is that
    features "flipped" when making adversarial examples are separate from the
    ones actually used to classify inputs.)

</li> 
<li><a name="world2"></a>
    <b>World 2: Adversarial examples exploit features that are useful for
    classification.</b> In this world, adversarial perturbations
can correspond to changes in the input that manipulate features relevant to
classification. Thus, models base their (mostly correct) predictions on
features that can be altered via small perturbations.</li> 
    </ul> 
  </p>

  <p> 
  Recent works provide some theoretical evidence that adversarial examples
  can arise from finite-sample overfitting [1] or other concentration of
  measure-based phenomena [2], thus supporting the “World 1” viewpoint on
  adversarial examples. The question is: is “World 1” the right way to
  think about adversarial examples? If so, this would be good news---under
  this mindset, adversarial robustness might just be a matter of getting
  better, “bug-free” models (for example, by reducing overfitting).

  Surprisingly, our findings show that the “World 1” mindset alone does not
  fully capture adversarial vulnerability; World 2 must be taken into
  account. Adversarial examples can---and do, if generated via standard
  methods---rely on "flipping" features that are actually useful for
  classification. In particular, we show that by relying <i>only</i> on
  perturbations corresponding to standard first-order adversarial attacks
  one can learn models that generalize to the test set. This means that
  these perturbations truly correspond to directions that are relevant for
  classifying new, unmodified inputs from the dataset. In summary, our
  message is: </p>

<p style="text-align:center"><b>Adversarial vulnerability can arise from
    flipping features in the data that are useful for
    classification of <i>correct</i> inputs.</b></p>

<p>In particular, note that our experiments (training on the
\(\widehat{\mathcal{D}}_{rand}\) and \(\widehat{\mathcal{D}}_{det}\) datasets;
Section 3.2) would not have the same result in World 1. Concretely, in
our “cartoon example” presented above, the classifier puts large weight \(w\) on a feature
coordinate $f(x)$ that is not generalizing for “natural images.” Then,
adversarial examples towards either class can be made by simply making
$f(x)$ slightly positive or slightly negative. However, a classifier
learned from these adversarial examples would <i> not </i> generalize to
the true dataset (since it would learn to depend on a feature that is not
useful on natural images).</p>

<h3><i>Takeaway #2</i>: Learning from “meaningless” data </h3>
<p>Another implication of our experiments is that models may not even
<i>need</i> any information which we as humans view as "meaningful" in order
to do well (in the generalization sense) on standard image datasets. (Our
\(\widehat{\mathcal{D}}_{NR}\) dataset is a perfect example of this.)</p>

<a name="cannotpin"></a>
<h3><i>Takeaway #3</i>: Cannot fully attribute adversarial examples to X</h3>
<p>On the other hand, we also show that we cannot conclusively fully
attribute adversarial examples to any specific aspect of the standard
training framework (BatchNorm, ResNets, SGD, etc.). In particular, our
“robust dataset” is a counterexample to any claim of the form “given any
dataset, training with BatchNorm/SGD/ResNets/overparameterization/etc. leads
to adversarial vulnerability.” In that sense, the dataset clearly plays a
role in the emergence of adversarial examples. (Also, further corroborating
this is Preetum’s “adversarial squares” dataset [here](), where standard
networks do not become adversarially vulnerable when there is no label
noise or overfitting.)
</p>

<h2>A Few Clarifications</h2>
<p>In addition to further refining our understanding of adversarial examples,
the comments were also very useful in pointing out which aspects of our
claims could benefit from further clarification. To this end, we make these
clarifications below via a couple “non-claims”---claims that we did
<i>not</i> intend to make. We’ll also update our paper in order to make
these clarifications explicit.</p>

<h3>Non-Claim #1: "Adversarial examples <b>cannot</b> be bugs"</h3>
<p>
Our goal is to say that since adversarial examples can arise from
well-generalizing features, simply patching up the “bugs” in ML models will
not get rid of adversarial vulnerability---we also need to make sure our
models learn the right features. This, however, does not mean that
adversarial vulnerability <i>cannot</i> arise from “bugs”. In fact, note
that several papers (e.g. <dt-cite key="TODO"></dt-cite>, <dt-cite
    key="TODO"></dt-cite>) have proven that adversarial vulnerability can
arise from what we refer to as “bugs,” e.g. finite-sample overfitting,
concentration of measure, high dimensionality, etc. Furthermore,,
Preetum’s experiments below constitute a clean demonstration of adversarial
examples arising from “bugs” in even more realistic settings.
</p>
<p>We note that several papers (e.g. ,
<dt-cite key="TODO"></dt-cite>) have mathematically proven that adversarial
vulnerability can arise from what we refer to as “bugs,” e.g. finite-sample
overfitting, concentration of measure, high dimensionality, etc. Our paper
does not render these works invalid or disprove them in any way, and thus
we never meant to suggest that adversarial vulnerability <b> cannot </b>
arise from “bugs.” (In fact, Preetum’s experiments below are the first to
our knowledge to demonstrate adversarial examples arising from “bugs” in
even more realistic settings). Our goal is merely to say that since they
can arise from well-generalizing features, simply patching up the “bugs” in
ML models will not get rid of adversarial vulnerability---we also need to
make sure they learn the right features.</p>

<h3>Non-Claim #2: "Adversarial examples are purely a result of the dataset"</h3>
<p> Even though we <a href="#cannotpin">demonstrated</a> that datasets do
play a role in the emergence of adversarial examples, we do not intend to
claim that this role is exclusive.  In particular, just because the data
<i> admits </i> non-robust functions that are well-generalizing (useful
non-robust features), doesn’t mean that <i> any </i> model will learn to
pick up these features. For example, it could be that the well-generalizing
features that cause adversarial examples are only learnable by certain
architectures. On the other hand, we do show that there is a way, via only
altering the dataset, to induce robust models---thus, our results indicate
that adversarial vulnerability indeed cannot be completely disentangled
from the dataset (more on this in <a href="#cannotpin">Takeaway #3</a>).</p>

<h2>Responses to comments</h2>

<h3>Two Examples of Useful, Non-Robust Features (Gabriel Goh)</h3>
<p><b>Response Summary</b>: The construction of explicit non-robust
features is very interesting and makes progress towards the challenge of
visualizing some of the useful non-robust features detected by our
experiments. We also agree that non-robust features arising as
“distractors” is indeed not precluded by our theoretical framework, even
though it is precluded by our experiments. Though for our purposes this
simple theoretical framework sufficed for reasoning about
experiments\footnote{We also presented a theoretical setting where we can
analyze things fully rigorously in Section 4 of our paper}, this comment
identifies finding a more robust definition of feature as an interesting
future research direction.</p>

</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
</script>

